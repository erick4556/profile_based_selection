{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IW7PJarJNH19"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob import Word\n",
    "import pandas as pd\n",
    "import string as st\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pandas_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3UL6wPvPH_I",
    "outputId": "60f95775-b525-48ff-c43e-47c36a647756"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/profile_dataset2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2440, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories: ['business' 'entertainment' 'politics' 'sport' 'tech']\n"
     ]
    }
   ],
   "source": [
    "# Total type of unique categories\n",
    "print(\"Unique categories:\",df['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2342 entries, 0 to 2439\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   news    2342 non-null   object\n",
      " 1   type    2342 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 54.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Information about metadata\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = ProfileReport(df)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top categories by and number of articles per categories\n",
    "cat_df = pd.DataFrame(df['type'].value_counts()).reset_index()\n",
    "cat_df.rename(columns={'index':'news_classes','type':'numcat'}, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "ax = sns.barplot(np.array(cat_df.news_classes)[:5], np.array(cat_df.numcat)[:5])\n",
    "for p in ax.patches:\n",
    "    ax.annotate(p.get_height(), (p.get_x()+0.01, p.get_height() + 8))\n",
    "plt.title(\"TOP 5 Categories of News Articles\", size=15)\n",
    "plt.xlabel(\"Categories of articles\", size=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Number of articles\", size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'TOP 5 Categories of News Articles'}, xlabel='type', ylabel='count'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.countplot(x='type', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning and dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    return (\"\".join([ch for ch in text if ch not in st.punctuation]))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    #text = text.translate(str.maketrans('', '', st.punctuation))\n",
    "    text = remove_punct(text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    clean_text = ' '.join(tokens)\n",
    "\n",
    "    return clean_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r85LPzo-N2hF"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset):\n",
    "    #Divide in features and labels. We're gonna do a tokenization and lemmatization process with the news features\n",
    "    x = df['news'].tolist()\n",
    "    y = df['type'].tolist()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    print(\"Data preparation\")\n",
    "    print(\"Tokenization and Lemmatization process\", end='', flush=True)\n",
    "    for i, value in enumerate(x):\n",
    "        x[i] = ' '.join([wordnet_lemmatizer.lemmatize(word) for word in clean_text(value).split()])\n",
    "        \n",
    "    \n",
    "    #Train and test set split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "    vect = TfidfVectorizer(min_df=2)\n",
    "\n",
    "    #Training process\n",
    "    X_train = vect.fit_transform(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = vect.transform(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    #Validation and test set split\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.50, random_state=42)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Train set:\" + str(X_train.shape))\n",
    "    print(\"Validation set:\" + str(X_val.shape))\n",
    "    print(\"Test set:\" + str(X_test.shape))\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process for knowing how many neighbors we may use in the K-neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset2 = pd.read_csv('./datasets/profile_dataset2.csv', encoding=\"ISO-8859-1\")\n",
    "y = df['type'] #Variable objetivo a predecir\n",
    "X = df.drop(\"type\", axis = 1) #Quitar la columna de vehicle_class\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, vect = prepare_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process for knowing how many neighbors we may use\n",
    "error_rate = []\n",
    "for x in range(1,30):\n",
    "    knn = KNeighborsClassifier(n_neighbors=x)\n",
    "    knn.fit(X_train, y_train)\n",
    "    predictions_x = knn.predict(X_test)\n",
    "    #Agregar la media de los valores que encuentre diferente entre las predicciones y el valor real. \n",
    "    error_rate.append(np.mean(predictions_x != y_test))\n",
    "    \n",
    "error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe4f59813a0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = range(1,30)\n",
    "plt.plot(values, error_rate, color = \"green\", marker=\"o\", markerfacecolor=\"red\", markersize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0o-Kd4deOOJy"
   },
   "outputs": [],
   "source": [
    "def rf_train_model(X_train, y_train):\n",
    "    model = RandomForestClassifier(n_estimators=200, max_depth=150, n_jobs=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def svc_train_model(X_train, y_train):\n",
    "    model = SVC(kernel='linear', gamma='auto', probability=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def kn_train_model(X_train, y_train):\n",
    "    model = KNeighborsClassifier(n_neighbors=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluation_model(model, X_val, y_val):\n",
    "    y_pred = model.predict(X_val)\n",
    "    result = classification_report(y_val, y_pred)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    print(result)\n",
    "    print(\"\\nAccuracy: \", acc)\n",
    "\n",
    "def evaluation_test_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    result = classification_report(y_test, y_pred)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(result)\n",
    "    print(\"\\nAccuracy: \", acc)\n",
    "    \n",
    "def test_model_accuracy(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set users preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wee_qrw2OSbq"
   },
   "outputs": [],
   "source": [
    "topics = [\"sport\", \"tech\", \"business\", \"politics\", \"entertainment\"]\n",
    "\n",
    "# Set preferences by users\n",
    "def set_preferences(users):\n",
    "    user_preferences = {}\n",
    "    for user in users:\n",
    "        preferences = []\n",
    "        possibilities = list(topics)\n",
    "        for pref in range(random.randint(1, 3)):\n",
    "            category = random.choice(possibilities)\n",
    "            preferences.append(category)\n",
    "            possibilities.remove(category)\n",
    "        user_preferences[user] = preferences\n",
    "    return user_preferences\n",
    "\n",
    "# Predict the document type\n",
    "def predict_doc_type(doc, vect, model):\n",
    "    document_cleaned = clean_text(doc)\n",
    "    corpus = []\n",
    "    corpus.append(document_cleaned)\n",
    "    test_vect = vect.transform(corpus)\n",
    "    return model.predict(test_vect)[0]\n",
    "\n",
    "\n",
    "# Testing results\n",
    "def run_test(vect, model):\n",
    "    users = [\"Gabriela\", \"Pablo\", \"Erick\", \"Marco\", \"Sam\", \"Luis\"]\n",
    "    users_preferences = set_preferences(users)\n",
    "    print(\"\\n\")\n",
    "    print(\"User preferences:\")\n",
    "    for user in users_preferences:\n",
    "        print(user, \"likes these topics:\", users_preferences[user])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            test_corpus = input(\"Enter a text about a specific tpic. If you wanna reassign the preferences please type 'a' or if you wanna quit, please type 'e':\\n\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if test_corpus == \"e\":\n",
    "            break\n",
    "        if test_corpus == \"a\":\n",
    "            users_preferences = set_preferences(users)\n",
    "            print(\"\\n\")\n",
    "            print(\"User preferences:\")\n",
    "            for user in users_preferences:\n",
    "                print(user, \"likes these topics:\", users_preferences[user])\n",
    "            print(\"\\n\")\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "\n",
    "            result = predict_doc_type(test_corpus, vect, model)\n",
    "            if (result == \"sport\"):\n",
    "                topic = result.upper()\n",
    "            if (result == \"tech\"):\n",
    "                topic = result.upper()\n",
    "            if(result == \"business\"):\n",
    "                topic = result.upper()\n",
    "            if (result == \"entertainment\"):\n",
    "                topic = result.upper()\n",
    "            if (result == \"politics\"):\n",
    "                topic = result.upper()\n",
    "            print(\"\\n\")\n",
    "            print(\"This article talks about \"+ topic + \" \" + \"and it's addressed to:\")\n",
    "            for user in users_preferences:\n",
    "                if result in users_preferences[user]:\n",
    "                    print(user)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_percentage(value):\n",
    "    return f'{value:.2f}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_top_categories(X_test, model, top_n=5):\n",
    "    class_probabilities = model.predict_proba(X_test)\n",
    "    summed_probabilities = np.mean(class_probabilities, axis=0)\n",
    "    class_indices = np.argsort(summed_probabilities)[::-1][:top_n]\n",
    "    ranked_classes = [(model.classes_[i], summed_probabilities[i]*100) for i in class_indices]\n",
    "    rank_df = pd.DataFrame(ranked_classes, columns=['Category', 'Percentage'])\n",
    "    rank_df['Percentage'] = rank_df['Percentage'].apply(format_percentage)\n",
    "    for index, row in rank_df.iterrows():\n",
    "        print(f\"{row['Category']}: {row['Percentage']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "E5rBbKL-OXY5"
   },
   "outputs": [],
   "source": [
    "def test_project():\n",
    "\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, vect = prepare_dataset(df)\n",
    "        \n",
    "    #Models Calls\n",
    "    svc_model = svc_train_model(X_train, y_train)\n",
    "    rf_model = rf_train_model(X_train, y_train)\n",
    "    kn_model = kn_train_model(X_train, y_train)\n",
    "        \n",
    "    # Models training\n",
    "    print(\"\\n\")\n",
    "    print(\"-----Models Training-----\")\n",
    "    \n",
    "    # Suport Vector Machine\n",
    "    print(\"\\nSuport Vector Machine\")\n",
    "    evaluation_model(svc_model, X_val, y_val)\n",
    "    # Random Forest\n",
    "    print(\"\\nRandom Forest\")\n",
    "    evaluation_model(rf_model, X_val, y_val)\n",
    "    # K Neighbors\n",
    "    print(\"\\nK Neighbors\")\n",
    "    evaluation_model(kn_model, X_val, y_val)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    # Models testing\n",
    "    print(\"\\n\")\n",
    "    print(\"-----Models Testing-----\\n\")\n",
    "    print(\"\\nSuport Vector Machine\")\n",
    "    evaluation_test_model(svc_model, X_test, y_test)\n",
    "    print(\"\\nRandom Forest\")\n",
    "    evaluation_test_model(rf_model, X_test, y_test)\n",
    "    print(\"\\nK Neighbors\")\n",
    "    evaluation_test_model(kn_model, X_test, y_test)\n",
    "    print(\"\\n\")\n",
    "    print(\"\\nAccuracies Models\")\n",
    "    print(\"Suport Vector Machine:\", test_model_accuracy(svc_model, X_test, y_test))\n",
    "    print(\"Random Forest:\", test_model_accuracy(rf_model, X_test, y_test))\n",
    "    #print(\"Neural Network:\", test_model(nn_model, X_test, y_test))\n",
    "    print(\"K Neighbors:\", test_model_accuracy(kn_model, X_test, y_test))\n",
    "    print(\"------------------------------\")    \n",
    "    \n",
    "    print(\"\\n-----Ranking of Categories for Support Vector Machine-----\\n\")\n",
    "    rank_top_categories(X_test, svc_model)\n",
    "    \n",
    "    # Run test\n",
    "    #We pass the vector and any model we have trained\n",
    "    run_test(vect, svc_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L2w-pbQSO98d",
    "outputId": "6621f3b5-f040-4209-8459-8117d0cf3add"
   },
   "outputs": [],
   "source": [
    "test_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
