{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IW7PJarJNH19"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob import Word\n",
    "import pandas as pd\n",
    "import string as st\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3UL6wPvPH_I",
    "outputId": "60f95775-b525-48ff-c43e-47c36a647756"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\erick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning and dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r85LPzo-N2hF"
   },
   "outputs": [],
   "source": [
    "def text_cleaning(string):\n",
    "\n",
    "    #Clean string in a manual form\n",
    "    string = re.sub(r\"\\'s\", \"\", string)\n",
    "    string = re.sub(r\"\\'ve\", \"\", string)\n",
    "    string = re.sub(r\"n\\'t\", \"\", string)\n",
    "    string = re.sub(r\"\\'re\", \"\", string)\n",
    "    string = re.sub(r\"\\'d\", \"\", string)\n",
    "    string = re.sub(r\"\\'ll\", \"\", string)\n",
    "    string = re.sub(r\",\", \"\", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \"\", string)\n",
    "    string = re.sub(r\"\\)\", \"\", string)\n",
    "    string = re.sub(r\"\\?\", \"\", string)\n",
    "    string = re.sub(r\"'\", \"\", string)\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"[0-9]\\w+|[0-9]\", \"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "    return string.strip().lower()\n",
    "\n",
    "def remove_punct(text):\n",
    "    return (\"\".join([ch for ch in text if ch not in st.punctuation]))\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    #Divide in features and labels. We're gonna do a tokenization and lemmatization process with the news features\n",
    "    x = dataset['news'].tolist()\n",
    "    y = dataset['type'].tolist()\n",
    "    print(\"Data preparation\")\n",
    "    print(\"Tokenization and Lemmatization process\", end='', flush=True)\n",
    "    for i, value in enumerate(x):\n",
    "        x[i] = ' '.join([Word(word).lemmatize() for word in text_cleaning(value).split()])\n",
    "        \n",
    "    \n",
    "    #Train and test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "    vect = TfidfVectorizer(stop_words='english', min_df=2)\n",
    "\n",
    "    #Training process\n",
    "    X_train = vect.fit_transform(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = vect.transform(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    #Train and test split with the vector\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.50, random_state=42)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Train set:\" + str(X_train.shape))\n",
    "    print(\"Validation set:\" + str(X_val.shape))\n",
    "    print(\"Test set:\" + str(X_test.shape))\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process for knowing how many neighbors we may use in the K-neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation\n",
      "Tokenization and Lemmatization process\n",
      "\n",
      "Train set:(1952, 13347)\n",
      "Validation set:(244, 13347)\n",
      "Test set:(244, 13347)\n"
     ]
    }
   ],
   "source": [
    "dataset2 = pd.read_csv('./datasets/profile_dataset2.csv', encoding=\"ISO-8859-1\")\n",
    "y = dataset2['type'] #Variable objetivo a predecir\n",
    "X = dataset2.drop(\"type\", axis = 1) #Quitar la columna de vehicle_class\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, vect = prepare_dataset(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5614754098360656,\n",
       " 0.48770491803278687,\n",
       " 0.5245901639344263,\n",
       " 0.11885245901639344,\n",
       " 0.11065573770491803,\n",
       " 0.09836065573770492,\n",
       " 0.09016393442622951,\n",
       " 0.0942622950819672,\n",
       " 0.0860655737704918,\n",
       " 0.08196721311475409,\n",
       " 0.0860655737704918,\n",
       " 0.09836065573770492,\n",
       " 0.0942622950819672,\n",
       " 0.09016393442622951,\n",
       " 0.08196721311475409,\n",
       " 0.08196721311475409,\n",
       " 0.0778688524590164,\n",
       " 0.0860655737704918,\n",
       " 0.09016393442622951,\n",
       " 0.09016393442622951,\n",
       " 0.0860655737704918,\n",
       " 0.08196721311475409,\n",
       " 0.09016393442622951,\n",
       " 0.0860655737704918,\n",
       " 0.08196721311475409,\n",
       " 0.0778688524590164,\n",
       " 0.08196721311475409,\n",
       " 0.0860655737704918,\n",
       " 0.0778688524590164]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Process for knowing how many neighbors we may use\n",
    "error_rate = []\n",
    "for x in range(1,30):\n",
    "    knn = KNeighborsClassifier(n_neighbors=x)\n",
    "    knn.fit(X_train, y_train)\n",
    "    predictions_x = knn.predict(X_test)\n",
    "    #Agregar la media de los valores que encuentre diferente entre las predicciones y el valor real. \n",
    "    error_rate.append(np.mean(predictions_x != y_test))\n",
    "    \n",
    "error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24e18246370>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "values = range(1,30)\n",
    "plt.plot(values, error_rate, color = \"green\", marker=\"o\", markerfacecolor=\"red\", markersize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0o-Kd4deOOJy"
   },
   "outputs": [],
   "source": [
    "def NN_train_model(X_train, y_train):\n",
    "    print(\"\\nNeural Network\")\n",
    "    model = MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n",
    "              beta_1=0.9, beta_2=0.999, early_stopping=True,\n",
    "              epsilon=1e-08, hidden_layer_sizes=(128, 64),\n",
    "              learning_rate='constant', learning_rate_init=0.001,\n",
    "              max_iter=400, momentum=0.9, n_iter_no_change=10,\n",
    "              nesterovs_momentum=True, power_t=0.5, random_state=1,\n",
    "              shuffle=True, solver='lbfgs', tol=0.0001,\n",
    "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def RF_train_model(X_train, y_train):\n",
    "    print(\"\\nRandom Forest\")\n",
    "    model = RandomForestClassifier(n_estimators=200, max_depth=150, n_jobs=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def SVC_train_model(X_train, y_train):\n",
    "    print(\"\\nSuport Vector Machine\")\n",
    "    model = SVC(kernel='linear', gamma='auto')\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def KN_train_model(X_train, y_train):\n",
    "    print(\"\\nK Neighbor\")\n",
    "    model = KNeighborsClassifier(n_neighbors=10)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluation_model(model, X_val, y_val):\n",
    "    y_pred = model.predict(X_val)\n",
    "    result = classification_report(y_val, y_pred)\n",
    "    accSVC = accuracy_score(y_val, y_pred)\n",
    "    print(result)\n",
    "    print(\"\\nAccuracy: \", accSVC)\n",
    "\n",
    "\n",
    "def test_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set users preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wee_qrw2OSbq"
   },
   "outputs": [],
   "source": [
    "topics = [\"sport\", \"tech\", \"business\", \"politics\", \"entertainment\"]\n",
    "\n",
    "# Set preferences by users\n",
    "def set_preferences(users):\n",
    "    user_preferences = {}\n",
    "    for user in users:\n",
    "        preferences = []\n",
    "        possibilities = list(topics)\n",
    "        for pref in range(random.randint(1, 3)):\n",
    "            category = random.choice(possibilities)\n",
    "            preferences.append(category)\n",
    "            possibilities.remove(category)\n",
    "        user_preferences[user] = preferences\n",
    "    return user_preferences\n",
    "\n",
    "# Predict the document type\n",
    "def predict_doc_type(doc, vect, model):\n",
    "    document_cleaned = text_cleaning(doc)\n",
    "    corpus = []\n",
    "    corpus.append(document_cleaned)\n",
    "    test_vect = vect.transform(corpus)\n",
    "    return model.predict(test_vect)[0]\n",
    "\n",
    "\n",
    "# Testing results\n",
    "def run_test(vect, model):\n",
    "    users = [\"Gabriela\", \"Pablo\", \"Erick\", \"Marco\", \"Sam\", \"Luis\"]\n",
    "    users_preferences = set_preferences(users)\n",
    "    print(\"\\n\")\n",
    "    print(\"User preferences:\")\n",
    "    for user in users_preferences:\n",
    "        print(user, \"likes these topics:\", users_preferences[user])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            test_corpus = input(\"Enter a text about a specific tpic. If you wanna quit, please press 'e' or if you wanna reassign the preferences please press 'a':\\n\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if test_corpus == \"e\":\n",
    "            break\n",
    "        if test_corpus == \"a\":\n",
    "            users_preferences = set_preferences(users)\n",
    "            print(\"\\n\")\n",
    "            print(\"User preferences:\")\n",
    "            for user in users_preferences:\n",
    "                print(user, \"likes these topics:\", users_preferences[user])\n",
    "            print(\"\\n\")\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "\n",
    "            result = predict_doc_type(test_corpus, vect, model)\n",
    "            if (result == \"sport\"):\n",
    "                topic = result.upper()\n",
    "            if (result == \"tech\"):\n",
    "                topic = result.upper()\n",
    "            if(result == \"business\"):\n",
    "                topic = result.upper()\n",
    "            if (result == \"entertainment\"):\n",
    "                topic = result.upper()\n",
    "            if (result == \"politics\"):\n",
    "                topic = result.upper()\n",
    "            print(\"\\n\")\n",
    "            print(\"This article talks about \"+ topic + \" \" + \"and it's addressed to:\")\n",
    "            for user in users_preferences:\n",
    "                if result in users_preferences[user]:\n",
    "                    print(user)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "E5rBbKL-OXY5"
   },
   "outputs": [],
   "source": [
    "def test_project():\n",
    "    # Load dataset and data preparation\n",
    "    dataset = pd.read_csv('./datasets/profile_dataset2.csv', encoding=\"ISO-8859-1\")\n",
    "    print(\"Data\", dataset.shape)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, vect = prepare_dataset(dataset)\n",
    "\n",
    "    # Models training\n",
    "    print(\"\\n\")\n",
    "    print(\"Models Training\")\n",
    "    \n",
    "    # Suport Vector Machine\n",
    "    svc_model = SVC_train_model(X_train, y_train)\n",
    "    evaluation_model(svc_model, X_val, y_val)\n",
    "    # Random Forest\n",
    "    rf_model = RF_train_model(X_train, y_train)\n",
    "    evaluation_model(rf_model, X_val, y_val)\n",
    "    # Neural Network\n",
    "    nn_model = NN_train_model(X_train, y_train)\n",
    "    evaluation_model(nn_model, X_val, y_val)\n",
    "    # K Neighbor\n",
    "    kn_model = KN_train_model(X_train, y_train)\n",
    "    evaluation_model(kn_model, X_val, y_val)\n",
    "\n",
    "\n",
    "    # Models testing\n",
    "    print(\"\\n\")\n",
    "    print(\"Models testing\\n\")\n",
    "    print(\"Suport Vector Machine:\", test_model(svc_model, X_test, y_test))\n",
    "    print(\"Random Forest:\", test_model(rf_model, X_test, y_test))\n",
    "    print(\"Neural Network:\", test_model(nn_model, X_test, y_test))\n",
    "    print(\"K Neighbors:\", test_model(kn_model, X_test, y_test))\n",
    "\n",
    "\n",
    "    # Run test\n",
    "    #We pass the vector and any model we have trained\n",
    "    run_test(vect, svc_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L2w-pbQSO98d",
    "outputId": "6621f3b5-f040-4209-8459-8117d0cf3add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data (2440, 2)\n",
      "Data preparation\n",
      "Tokenization and Lemmatization process\n",
      "\n",
      "Train set:(1952, 13347)\n",
      "Validation set:(244, 13347)\n",
      "Test set:(244, 13347)\n",
      "\n",
      "\n",
      "Models Training\n",
      "\n",
      "Suport Vector Machine\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.95      0.92      0.94        63\n",
      "entertainment       1.00      0.95      0.97        39\n",
      "     politics       0.91      0.98      0.94        41\n",
      "        sport       0.97      0.98      0.97        58\n",
      "         tech       0.95      0.95      0.95        43\n",
      "\n",
      "     accuracy                           0.95       244\n",
      "    macro avg       0.96      0.96      0.96       244\n",
      " weighted avg       0.96      0.95      0.95       244\n",
      "\n",
      "\n",
      "Accuracy:  0.9549180327868853\n",
      "\n",
      "Random Forest\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.97      0.89      0.93        63\n",
      "entertainment       0.97      0.92      0.95        39\n",
      "     politics       0.91      0.98      0.94        41\n",
      "        sport       0.95      1.00      0.97        58\n",
      "         tech       0.91      0.93      0.92        43\n",
      "\n",
      "     accuracy                           0.94       244\n",
      "    macro avg       0.94      0.94      0.94       244\n",
      " weighted avg       0.94      0.94      0.94       244\n",
      "\n",
      "\n",
      "Accuracy:  0.9426229508196722\n",
      "\n",
      "Neural Network\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.95      0.89      0.92        63\n",
      "entertainment       1.00      0.95      0.97        39\n",
      "     politics       0.83      0.98      0.90        41\n",
      "        sport       0.98      0.97      0.97        58\n",
      "         tech       0.95      0.95      0.95        43\n",
      "\n",
      "     accuracy                           0.94       244\n",
      "    macro avg       0.94      0.95      0.94       244\n",
      " weighted avg       0.95      0.94      0.94       244\n",
      "\n",
      "\n",
      "Accuracy:  0.9426229508196722\n",
      "\n",
      "K Neighbor\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.92      0.90      0.91        63\n",
      "entertainment       1.00      0.90      0.95        39\n",
      "     politics       0.86      0.90      0.88        41\n",
      "        sport       0.95      0.98      0.97        58\n",
      "         tech       0.91      0.93      0.92        43\n",
      "\n",
      "     accuracy                           0.93       244\n",
      "    macro avg       0.93      0.92      0.92       244\n",
      " weighted avg       0.93      0.93      0.93       244\n",
      "\n",
      "\n",
      "Accuracy:  0.9262295081967213\n",
      "\n",
      "\n",
      "Models testing\n",
      "\n",
      "Suport Vector Machine: 0.9549180327868853\n",
      "Random Forest: 0.930327868852459\n",
      "Neural Network: 0.9426229508196722\n",
      "K Neighbors: 0.9180327868852459\n",
      "\n",
      "\n",
      "User preferences:\n",
      "Gabriela likes these topics: ['business', 'entertainment', 'politics']\n",
      "Pablo likes these topics: ['entertainment', 'tech', 'business']\n",
      "Erick likes these topics: ['entertainment']\n",
      "Marco likes these topics: ['entertainment']\n",
      "Sam likes these topics: ['tech', 'business', 'entertainment']\n",
      "Luis likes these topics: ['politics']\n",
      "\n",
      "\n",
      "Enter a text about a specific tpic. If you wanna quit, please press 'e' or if you wanna reassign the preferences please press 'a':\n",
      "a\n",
      "\n",
      "\n",
      "User preferences:\n",
      "Gabriela likes these topics: ['tech', 'politics', 'sport']\n",
      "Pablo likes these topics: ['business', 'tech', 'sport']\n",
      "Erick likes these topics: ['politics', 'entertainment', 'sport']\n",
      "Marco likes these topics: ['tech']\n",
      "Sam likes these topics: ['business']\n",
      "Luis likes these topics: ['entertainment', 'politics', 'tech']\n",
      "\n",
      "\n",
      "Enter a text about a specific tpic. If you wanna quit, please press 'e' or if you wanna reassign the preferences please press 'a':\n",
      "a\n",
      "\n",
      "\n",
      "User preferences:\n",
      "Gabriela likes these topics: ['tech', 'business', 'politics']\n",
      "Pablo likes these topics: ['politics']\n",
      "Erick likes these topics: ['business']\n",
      "Marco likes these topics: ['politics']\n",
      "Sam likes these topics: ['business']\n",
      "Luis likes these topics: ['business', 'sport']\n",
      "\n",
      "\n",
      "Enter a text about a specific tpic. If you wanna quit, please press 'e' or if you wanna reassign the preferences please press 'a':\n",
      "a\n",
      "\n",
      "\n",
      "User preferences:\n",
      "Gabriela likes these topics: ['entertainment']\n",
      "Pablo likes these topics: ['entertainment']\n",
      "Erick likes these topics: ['entertainment', 'politics', 'sport']\n",
      "Marco likes these topics: ['business', 'sport']\n",
      "Sam likes these topics: ['tech', 'politics', 'sport']\n",
      "Luis likes these topics: ['politics']\n",
      "\n",
      "\n",
      "Enter a text about a specific tpic. If you wanna quit, please press 'e' or if you wanna reassign the preferences please press 'a':\n",
      "a\n",
      "\n",
      "\n",
      "User preferences:\n",
      "Gabriela likes these topics: ['business', 'entertainment']\n",
      "Pablo likes these topics: ['politics']\n",
      "Erick likes these topics: ['entertainment', 'tech']\n",
      "Marco likes these topics: ['politics', 'entertainment', 'sport']\n",
      "Sam likes these topics: ['sport']\n",
      "Luis likes these topics: ['politics', 'tech']\n",
      "\n",
      "\n",
      "Enter a text about a specific tpic. If you wanna quit, please press 'e' or if you wanna reassign the preferences please press 'a':\n",
      "a\n",
      "\n",
      "\n",
      "User preferences:\n",
      "Gabriela likes these topics: ['business']\n",
      "Pablo likes these topics: ['sport', 'entertainment', 'tech']\n",
      "Erick likes these topics: ['politics', 'business', 'entertainment']\n",
      "Marco likes these topics: ['politics', 'sport']\n",
      "Sam likes these topics: ['business', 'entertainment', 'politics']\n",
      "Luis likes these topics: ['business', 'sport']\n",
      "\n",
      "\n",
      "Enter a text about a specific tpic. If you wanna quit, please press 'e' or if you wanna reassign the preferences please press 'a':\n",
      "Messi is the best player in the world\n",
      "\n",
      "\n",
      "This article talks about SPORT and it's addressed to:\n",
      "Pablo\n",
      "Marco\n",
      "Luis\n",
      "Enter a text about a specific tpic. If you wanna quit, please press 'e' or if you wanna reassign the preferences please press 'a':\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "test_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwH36RQeJGJm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
